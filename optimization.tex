% Copyright (C) 2013  Jim Turner
%
% This work is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License. To
% view a copy of this license, visit http://creativecommons.org/licenses/by-sa/3.0/deed.en_US.

\documentclass{article}
\title{Engineering Design Optimization}

\input{preamble.tex}

\begin{document}

\input{firstpage.tex}

\section{Standard Form}\label{sec:standard-form}

Find \(\vec{x} =
\begin{Bmatrix}
  x_1 \\
  x_2 \\
  \vdots \\
  x_n
\end{Bmatrix}\)
which minimizes \(f(\vec{x})\) subject to the constraints
\(\begin{aligned}
  g_j(\vec{x}) &\le 0, \ j = 1, 2, \dots, m \\
  h_k(\vec{x}) &\le 0, \ k = 1, 2, \dots, l
\end{aligned}\)

\section{Conditions for Optimality}\label{sec:conditions-optimality}

\subsection{Unconstrained}\label{sec:unconstrained-conditions}

\begin{description*}
  \item[Necessary conditions]~
    \begin{itemize*}
      \item \(\del F(x) = 0\)
      \item \(H(x)\) is positive semi-definite or positive definite
    \end{itemize*}
  \item[Sufficient conditions]~
    \begin{itemize*}
      \item \(H(x)\) is positive definite
    \end{itemize*}
\end{description*}

\subsection{Constrained}\label{sec:constrained-conditions}

Kuhn--Tucker Conditions are necessary for a relative minimum at \(\vec{x}^*\) and are sufficient to
ensure a global minimum at \(\vec{x}^*\) for convex programming problems:
\begin{enumerate*}
  \item \(\vec{x}^*\) is feasible (meets constraints)
  \item \(\lambda_j g_j(\vec{x}^*) = 0\) where \(j = 1, 2, \dots, m\) and \(\lambda_j \ge 0\)
  \item \(0 = \del f(\vec{x}^*) + \sum_{j=1}^m{\lambda_j \del g_j(\vec{x}^*)} +
    \sum_{k=1}^l{\lambda_{m+k} \del h_k(\vec{x}^*)}\)
    where \(\lambda_{m+k}\) are unrestricted in sign
\end{enumerate*}
To solve for \(\lambda_j\) for problems with only inequality constraints, let
\[\vec{B} = \del f(\vec{x}^*) \textrm{ and } A =
\begin{bmatrix}
  \del g_1(\vec{x}^*) & \del g_2(\vec{x}^*) & \cdots & \del g_n(\vec{x}^*)
\end{bmatrix}\]
where \(A\) only contains active constraints, then
\[\vec{\lambda} = -{\left[A^\top A\right]}^{-1} A^\top \vec{B}\]

\section{1-D Optimization}\label{sec:1-d-optimization}

\subsection{Bounding\slash{}Range Finding}\label{sec:bounding}

\begin{enumerate*}
\item \label{itm:choose-init-and-step} Choose \(x^0\) and a step size \(\Delta\). Set \(k = 0\). The
  larger the value of \(\Delta\), the fewer the function calls required for bounding but the larger
  the final bounds will be.
\item If \(f(x^0 - \abs{\Delta}) \ge f(x^0) \ge f(x^0 + \abs{\Delta})\),
  then \(\Delta\) is positive \\
  else if \(f(x^0 - \abs{\Delta}) \le f(x^0) \le f(x^0 + \abs{\Delta})\),
  then \(\Delta\) is negative \\
  else go to step~\ref{itm:choose-init-and-step}.
\item \label{itm:calc-next-x} \(x^{k+1} = x^k + 2^k \Delta\)
\item If \(f(x^{k+1}) < f(x^k)\),
  then \(k := k + 1\) and go to step~\ref{itm:calc-next-x} \\
  else the minimum is between \((x^{k-1}, x^{k+1})\)
\end{enumerate*}

\subsection{Interpolation\slash{}Approximation Methods}\label{sec:interp-approx}

Interpolation\slash{}approximation methods approximate the objective function with simpler functions
for which the minima are known, then iteratively improve their approximations. They are more
unpredictable and less robust than region elimination methods but converge faster for continuous and
well-conditioned problems.

\subsubsection{Three-Point Quadratic Non-Derivative Optimization with Refinement}\label{sec:quadratic-interpolation}

\begin{enumerate*}
\item Start with three points: \((x_1, f_1)\), \((x_2, f_2)\), \((x_3, f_3)\) where \(x_1\),
  \(x_3\) bound the minimum and \(x_2 \in (x_1, x_3)\). Typically, \(x_2 = \frac12(x_1 + x_3)\).
\item \label{itm:define-interp} Define an interpolation function \(\tilde{f}(x) = a_0 + a_1(x - x_1)
  + a_2(x - x_1)(x - x_2) \approx f(x)\)
\item Find \(f_\mrm{min} = \min{(f_1, f_2, f_3)}\) and associated \(x_\mrm{min}\).
\item To find the interpolation function, \(\tilde{f}(x)\):
\begin{align*}
  a_0 &= f_1 \\
  a_1 &= \frac{f_2 - f_1}{x_2 - x_1} \\
  a_2 &= \frac{1}{x_3 - x_2}\left(\frac{f_3 - f_1}{x_3 - x_1} - \frac{f_2 - f_1}{x_2 - x_1}\right)
\end{align*}
Then at the minimum of \(\tilde{f}(x)\), \(\left.\pd{\tilde{f}}{x}\right|_{\tilde{x}^*} = a_1 +
a_2(\tilde{x}^* - x_2) + a_2(\tilde{x}^* - x_1) = 0\). Solving for \(\tilde{x}^*\):
\begin{align*}
\tilde{x}^* &= \frac{x_2 + x_1}{2} - \frac{a_1}{2a_2} \\
\tilde{f}^* &= f(\tilde{x}^*)
\end{align*}
\item Check convergence and if converged, stop:
\[\abs{\frac{\tilde{x}^* - x_\mrm{min}}{x_\mrm{min}}} \le \epsilon_x \qquad \qquad
\abs{\frac{\tilde{f}^* - f_\mrm{min}}{f_\mrm{min}}} \le \epsilon_f\]
\item Save the best point (either \(\tilde{x}^*\) or \(x_\mrm{min}\)) and the two points that
  bracket it. Relabel the saved points as \(x_1\), \(x_2\), \(x_3\). Recalculate \(f_1\), \(f_2\),
  \(f_3\). Go to step~\ref{itm:define-interp}.
\end{enumerate*}

\subsubsection{Newton--Raphson Method}\label{sec:newton-raphson}

This method uses first and second derivative information to speed up convergence. However,
discontinuities are a problem, for some problems it can be expensive to get the derivative, and the
algorithm can diverge in some cases.

\begin{enumerate*}
\item Select a value \(x^0\).
\item \label{itm:newton-linear-approx} Build a linear approximation of \(f'\):
\[\tilde{f}'(x) = f'(x^k) + f''(x^k)(x - x^k)\]
\item Solve \(\tilde{f}'(x^{k+1}) = 0\) for \(x^{k+1}\):
\[x^{k+1} = x^k - \frac{f'(x^k)}{f''(x^k)}\]
\item Check for convergence (when \(\abs{x^{k+1} - x^k}\) is small). If not converged, go to
  step~\ref{itm:newton-linear-approx}.
\end{enumerate*}

\subsubsection{Two-Point Cubic Optimization with Refinement}\label{sec:cubic-interpolation}

This interpolation-based method uses first derivative information to generate splines for
\(\tilde{f}(x)\) and \(\tilde{f}'(x)\).

\subsection{Region Elimination Methods}\label{sec:region-elimination}

Region elimination methods iteratively eliminate subintervals of the design space from
consideration. They generally eliminate the same percentage of the space on each iteration. They are
more robust than interpolation\slash{}approximation method, particularly for discontinuous or
ill-conditioned functions, but may be slower for well-conditioned problems.

\subsubsection{Interval Halving Method}\label{sec:interval-halving}

After \(n\) calls to \(f\), the space is reduced to about \({\left(\frac12\right)}^{n/2}\) of its
original size.

\begin{enumerate*}
\setcounter{enumi}{-1}
\item Should already know \((x_\mrm{L}, f_\mrm{L})\) and \((x_\mrm{R}, f_\mrm{R})\) from bounding
  algorithm.
\item Calculate the following:
  \begin{align}
    x_\mrm{m} &= \frac{x_\mrm{L} + x_\mrm{R}}{2} \nonumber \\
    f_\mrm{m} &= f(x_\mrm{m}) \nonumber \\
    L &= x_\mrm{R} - x_\mrm{L} \label{eq:interval-halving-L}
  \end{align}
\item \label{itm:update-x1-x2} Calculate the following:
  \begin{align*}
    x_1 &= x_\mrm{L} + \frac{L}{4} \\
    f_1 &= f(x_1) \\
    x_2 &= x_\mrm{R} - \frac{L}{4} \\
    f_2 &= f(x_2)
  \end{align*}
\item Set \(x_\mrm{L}\), \(x_\mrm{m}\), and \(x_\mrm{R}\) to the three points that make a V shape.
\item Compute \(L\) using equation~\ref{eq:interval-halving-L} and check convergence. Go to
  step~\ref{itm:update-x1-x2} if not converged.
\end{enumerate*}

\subsubsection{Golden Section Method}\label{sec:golden-section-region}

Define the golden ratio conjugate as \(\Phi = \frac{\sqrt5-1}{2} \approx 0.61803\). After \(n\)
calls to \(f\), the space is reduced to about \(\Phi^n\) of its original size.

\begin{enumerate*}
\setcounter{enumi}{-1}
\item Should already know \((x_\mrm{L}, f_\mrm{L})\) and \((x_\mrm{R}, f_\mrm{R})\) from bounding
  algorithm.
\item Calculate the following:
  \begin{align}
    x_1 &= \Phi x_\mrm{L} + (1-\Phi) x_\mrm{R} \label{eq:golden-section-x1} \\
    f_1 &= f(x_1) \label{eq:golden-section-f1} \\
    x_2 &= (1-\Phi) x_\mrm{L} + \Phi x_\mrm{R} \label{eq:golden-section-x2} \\
    f_2 &= f(x_2) \label{eq:golden-section-f2}
  \end{align}
\item \label{itm:update-golden-section} If \(f_2 > f_1\), then
  \begin{itemize*}
  \item \(x_\mrm{R} := x_2\), \(f_\mrm{R} := f_2\)
  \item \(x_2 := x_1\), \(f_2 := f_1\)
  \item Compute the new values of \(x_1\), \(f_1\) using
    equations~\ref{eq:golden-section-x1}~and~\ref{eq:golden-section-f1}.
  \end{itemize*}
  else
  \begin{itemize*}
  \item \(x_\mrm{L} := x_1\), \(f_\mrm{L} := f_1\)
  \item \(x_1 := x_2\), \(f_1 := f_2\)
  \item Compute the new values of \(x_2\), \(f_2\) using
    equations~\ref{eq:golden-section-x2}~and~\ref{eq:golden-section-f2}.
  \end{itemize*}
\item Compute \(\epsilon = \frac{x_\mrm{R} - x_\mrm{L}}{L_0}\) and check convergence. If not
  converged, go to step~\ref{itm:update-golden-section}.
\end{enumerate*}

\subsubsection{Bisection Method}\label{sec:bisection-region}

This method uses first derivative information to eliminate half of the space on each iteration.

\begin{enumerate*}
\setcounter{enumi}{-1}
\item Should already know \((x_\mrm{L}, f_\mrm{L})\) and \((x_\mrm{R}, f_\mrm{R})\) from bounding
  algorithm. Also compute \(f'_\mrm{L} < 0\) and \(f'_\mrm{R} > 0\).
\item \label{itm:update-bisection-xm} Compute \(x_\mrm{m} = \frac{x_\mrm{L} + x_\mrm{R}}{2}\).
\item Compute \(f_\mrm{m} = f(x_\mrm{m})\) and \(f'_\mrm{m} = f'(x_\mrm{m})\).
\item If \(f'_\mrm{m} > 0\), then \(x_\mrm{R} := x_\mrm{m}\), else \(x_\mrm{L} := x_\mrm{m}\).
\item Compute \(\epsilon = \frac{x_\mrm{R} - x_\mrm{L}}{L_0}\) and check convergence. If not
  converged, go to step~\ref{itm:update-bisection-xm}.
\end{enumerate*}

\subsection{Hybrid Methods}

Often, the best general method is to use a region elimination method for a few iterations to reduce
the size of the bounds, then use an interpolation\slash{}approximation method to quickly converge on
the minimum.

\begin{thebibliography}{99}
\bibitem{flec} Ferguson, S. M., 2014, \emph{Engineering Design Optimization}, MAE 531 course
  lectures, North Carolina State University, Raleigh, NC.
\bibitem{eotap} Rao, S. S., 2009, \emph{Engineering Optimization: Theory and Practice}, 4th ed.,
  John Wiley \& Sons, Inc., Hoboken, NJ.
\end{thebibliography}

\end{document}
