% Copyright (C) 2013  Jim Turner
%
% This work is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License. To
% view a copy of this license, visit http://creativecommons.org/licenses/by-sa/3.0/deed.en_US.

\documentclass{article}
\title{Engineering Design Optimization}

\input{preamble.tex}

\begin{document}

\input{firstpage.tex}

\section{Standard Form}\label{sec:standard-form}

Find \(\vec{x} =
\begin{Bmatrix}
  x_1 \\
  x_2 \\
  \vdots \\
  x_n
\end{Bmatrix}\)
which minimizes \(f(\vec{x})\) subject to the constraints
\(\begin{aligned}
  g_j(\vec{x}) &\le 0, \ j = 1, 2, \dots, m \\
  h_k(\vec{x}) &\le 0, \ k = 1, 2, \dots, l
\end{aligned}\)

\section{Conditions for Optimality}\label{sec:conditions-optimality}

\subsection{Unconstrained}\label{sec:unconstrained-conditions}

\begin{description*}
  \item[Necessary conditions]~
    \begin{itemize*}
      \item \(\del F(x) = 0\)
      \item \(H(x)\) is positive semi-definite or positive definite
    \end{itemize*}
  \item[Sufficient conditions]~
    \begin{itemize*}
      \item \(H(x)\) is positive definite
    \end{itemize*}
\end{description*}

\subsection{Constrained}\label{sec:constrained-conditions}

Kuhn--Tucker Conditions are necessary for a relative minimum at \(\vec{x}^*\) and are sufficient to
ensure a global minimum at \(\vec{x}^*\) for convex programming problems:
\begin{enumerate*}
  \item \(\vec{x}^*\) is feasible (meets constraints)
  \item \(\lambda_j g_j(\vec{x}^*) = 0\) where \(j = 1, 2, \dots, m\) and \(\lambda_j \ge 0\)
  \item \(0 = \del f(\vec{x}^*) + \sum_{j=1}^m{\lambda_j \del g_j(\vec{x}^*)} +
    \sum_{k=1}^l{\lambda_{m+k} \del h_k(\vec{x}^*)}\)
    where \(\lambda_{m+k}\) are unrestricted in sign
\end{enumerate*}
To solve for \(\lambda_j\) for problems with only inequality constraints, let
\[\vec{B} = \del f(\vec{x}^*) \textrm{ and } A =
\begin{bmatrix}
  \del g_1(\vec{x}^*) & \del g_2(\vec{x}^*) & \cdots & \del g_n(\vec{x}^*)
\end{bmatrix}\]
where \(A\) only contains active constraints, then
\[\vec{\lambda} = -{\left[\transpose{A} A\right]}^{-1} \transpose{A} \vec{B}\]

\section{1-D Optimization}\label{sec:1-d-optimization}

\subsection{Bounding\slash{}Range Finding}\label{sec:bounding}

\begin{enumerate*}
\item \label{itm:choose-init-and-step} Choose \(x^0\) and a step size \(\Delta\). Set \(k = 0\). The
  larger the value of \(\Delta\), the fewer the function calls required for bounding but the larger
  the final bounds will be.
\item If \(f(x^0 - \abs{\Delta}) \ge f(x^0) \ge f(x^0 + \abs{\Delta})\),
  then \(\Delta\) is positive \\
  else if \(f(x^0 - \abs{\Delta}) \le f(x^0) \le f(x^0 + \abs{\Delta})\),
  then \(\Delta\) is negative \\
  else go to step~\ref{itm:choose-init-and-step}.
\item \label{itm:calc-next-x} \(x^{k+1} = x^k + 2^k \Delta\)
\item If \(f(x^{k+1}) < f(x^k)\),
  then \(k := k + 1\) and go to step~\ref{itm:calc-next-x} \\
  else the minimum is between \((x^{k-1}, x^{k+1})\)
\end{enumerate*}

\subsection{Interpolation\slash{}Approximation Methods}\label{sec:interp-approx}

Interpolation\slash{}approximation methods approximate the objective function with simpler functions
for which the minima are known, then iteratively improve their approximations. They are more
unpredictable and less robust than region elimination methods but converge faster for continuous and
well-conditioned problems.

\subsubsection{Three-Point Quadratic Non-Derivative Optimization with Refinement}\label{sec:quadratic-interpolation}

\begin{enumerate*}
\item Start with three points: \((x_1, f_1)\), \((x_2, f_2)\), \((x_3, f_3)\) where \(x_1\),
  \(x_3\) bound the minimum and \(x_2 \in (x_1, x_3)\). Typically, \(x_2 = \frac12(x_1 + x_3)\).
\item \label{itm:define-interp} Define an interpolation function \(\tilde{f}(x) = a_0 + a_1(x - x_1)
  + a_2(x - x_1)(x - x_2) \approx f(x)\)
\item Find \(f_\mrm{min} = \min{(f_1, f_2, f_3)}\) and associated \(x_\mrm{min}\).
\item To find the interpolation function, \(\tilde{f}(x)\):
\begin{align*}
  a_0 &= f_1 \\
  a_1 &= \frac{f_2 - f_1}{x_2 - x_1} \\
  a_2 &= \frac{1}{x_3 - x_2}\left(\frac{f_3 - f_1}{x_3 - x_1} - \frac{f_2 - f_1}{x_2 - x_1}\right)
\end{align*}
Then at the minimum of \(\tilde{f}(x)\), \(\left.\pd{\tilde{f}}{x}\right|_{\tilde{x}^*} = a_1 +
a_2(\tilde{x}^* - x_2) + a_2(\tilde{x}^* - x_1) = 0\). Solving for \(\tilde{x}^*\):
\begin{align*}
\tilde{x}^* &= \frac{x_2 + x_1}{2} - \frac{a_1}{2a_2} \\
\tilde{f}^* &= f(\tilde{x}^*)
\end{align*}
\item Check convergence and if converged, stop:
\[\abs{\frac{\tilde{x}^* - x_\mrm{min}}{x_\mrm{min}}} \le \epsilon_x \qquad \qquad
\abs{\frac{\tilde{f}^* - f_\mrm{min}}{f_\mrm{min}}} \le \epsilon_f\]
\item Save the best point (either \(\tilde{x}^*\) or \(x_\mrm{min}\)) and the two points that
  bracket it. Relabel the saved points as \(x_1\), \(x_2\), \(x_3\). Recalculate \(f_1\), \(f_2\),
  \(f_3\). Go to step~\ref{itm:define-interp}.
\end{enumerate*}

\subsubsection{Newton--Raphson Method}\label{sec:newton-raphson}

This method uses first and second derivative information to speed up convergence. However,
discontinuities are a problem, for some problems it can be expensive to get the derivative, and the
algorithm can diverge in some cases.

\begin{enumerate*}
\item Select a value \(x^0\).
\item \label{itm:newton-linear-approx} Build a linear approximation of \(f'\):
\[\tilde{f}'(x) = f'(x^k) + f''(x^k)(x - x^k)\]
\item Solve \(\tilde{f}'(x^{k+1}) = 0\) for \(x^{k+1}\):
\[x^{k+1} = x^k - \frac{f'(x^k)}{f''(x^k)}\]
\item Check for convergence (when \(\abs{x^{k+1} - x^k}\) is small). If not converged, go to
  step~\ref{itm:newton-linear-approx}.
\end{enumerate*}

\subsubsection{Two-Point Cubic Optimization with Refinement}\label{sec:cubic-interpolation}

This interpolation-based method uses first derivative information to generate splines for
\(\tilde{f}(x)\) and \(\tilde{f}'(x)\).

\subsection{Region Elimination Methods}\label{sec:region-elimination}

Region elimination methods iteratively eliminate subintervals of the design space from
consideration. They generally eliminate the same percentage of the space on each iteration. They are
more robust than interpolation\slash{}approximation method, particularly for discontinuous or
ill-conditioned functions, but may be slower for well-conditioned problems.

\subsubsection{Interval Halving Method}\label{sec:interval-halving}

After \(n\) calls to \(f\), the space is reduced to about \({\left(\frac12\right)}^{n/2}\) of its
original size.

\begin{enumerate*}
\setcounter{enumi}{-1}
\item Should already know \((x_\mrm{L}, f_\mrm{L})\) and \((x_\mrm{R}, f_\mrm{R})\) from bounding
  algorithm.
\item Calculate the following:
  \begin{align}
    x_\mrm{m} &= \frac{x_\mrm{L} + x_\mrm{R}}{2} \nonumber \\
    f_\mrm{m} &= f(x_\mrm{m}) \nonumber \\
    L &= x_\mrm{R} - x_\mrm{L} \label{eq:interval-halving-L}
  \end{align}
\item \label{itm:update-x1-x2} Calculate the following:
  \begin{align*}
    x_1 &= x_\mrm{L} + \frac{L}{4} \\
    f_1 &= f(x_1) \\
    x_2 &= x_\mrm{R} - \frac{L}{4} \\
    f_2 &= f(x_2)
  \end{align*}
\item Set \(x_\mrm{L}\), \(x_\mrm{m}\), and \(x_\mrm{R}\) to the three points that make a V shape.
\item Compute \(L\) using equation~\ref{eq:interval-halving-L} and check convergence. Go to
  step~\ref{itm:update-x1-x2} if not converged.
\end{enumerate*}

\subsubsection{Golden Section Method}\label{sec:golden-section-region}

Define the golden ratio conjugate as \(\Phi = \frac{\sqrt5-1}{2} \approx 0.61803\). After \(n\)
calls to \(f\), the space is reduced to about \(\Phi^n\) of its original size.

\begin{enumerate*}
\setcounter{enumi}{-1}
\item Should already know \((x_\mrm{L}, f_\mrm{L})\) and \((x_\mrm{R}, f_\mrm{R})\) from bounding
  algorithm.
\item Calculate the following:
  \begin{align}
    x_1 &= \Phi x_\mrm{L} + (1-\Phi) x_\mrm{R} \label{eq:golden-section-x1} \\
    f_1 &= f(x_1) \label{eq:golden-section-f1} \\
    x_2 &= (1-\Phi) x_\mrm{L} + \Phi x_\mrm{R} \label{eq:golden-section-x2} \\
    f_2 &= f(x_2) \label{eq:golden-section-f2}
  \end{align}
\item \label{itm:update-golden-section} If \(f_2 > f_1\), then
  \begin{itemize*}
  \item \(x_\mrm{R} := x_2\), \(f_\mrm{R} := f_2\)
  \item \(x_2 := x_1\), \(f_2 := f_1\)
  \item Compute the new values of \(x_1\), \(f_1\) using
    equations~\ref{eq:golden-section-x1}~and~\ref{eq:golden-section-f1}.
  \end{itemize*}
  else
  \begin{itemize*}
  \item \(x_\mrm{L} := x_1\), \(f_\mrm{L} := f_1\)
  \item \(x_1 := x_2\), \(f_1 := f_2\)
  \item Compute the new values of \(x_2\), \(f_2\) using
    equations~\ref{eq:golden-section-x2}~and~\ref{eq:golden-section-f2}.
  \end{itemize*}
\item Compute \(\epsilon = \frac{x_\mrm{R} - x_\mrm{L}}{L_0}\) and check convergence. If not
  converged, go to step~\ref{itm:update-golden-section}.
\end{enumerate*}

\subsubsection{Bisection Method}\label{sec:bisection-region}

This method uses first derivative information to eliminate half of the space on each iteration.

\begin{enumerate*}
\setcounter{enumi}{-1}
\item Should already know \((x_\mrm{L}, f_\mrm{L})\) and \((x_\mrm{R}, f_\mrm{R})\) from bounding
  algorithm. Also compute \(f'_\mrm{L} < 0\) and \(f'_\mrm{R} > 0\).
\item \label{itm:update-bisection-xm} Compute \(x_\mrm{m} = \frac{x_\mrm{L} + x_\mrm{R}}{2}\).
\item Compute \(f_\mrm{m} = f(x_\mrm{m})\) and \(f'_\mrm{m} = f'(x_\mrm{m})\).
\item If \(f'_\mrm{m} > 0\), then \(x_\mrm{R} := x_\mrm{m}\), else \(x_\mrm{L} := x_\mrm{m}\).
\item Compute \(\epsilon = \frac{x_\mrm{R} - x_\mrm{L}}{L_0}\) and check convergence. If not
  converged, go to step~\ref{itm:update-bisection-xm}.
\end{enumerate*}

\subsection{Hybrid Methods}

Often, the best general method is to use a region elimination method for a few iterations to reduce
the size of the bounds, then use an interpolation\slash{}approximation method to quickly converge on
the minimum.

\section{\textit{n}-D Optimization}

Unconstrained \(n\)-D optimization methods are typically formulated as a series of 1-D searches. The
individual methods determine the direction \(\vec{S}^q\) for each search. The update relation is
typically written as the following for iteration \(q\):
\[\vec{x}^{q+1} = \vec{x}^q + \alpha^* \vec{S}^q\]
where the 1-D optimization varies \(\alpha^*\) to find the minimum of \(F(\vec{x}^q + \alpha^*
\vec{S}^q)\).

\subsection{Zero Order Methods}

\subsubsection{Brute Force}

Don't actually use this method unless your objective function is very fast to evaluate.

\begin{enumerate*}
\item Pick a base point \(\vec{x}^0\) and set \(q = 0\).
\item \label{itm:brute-force-eval} Evaluate \(F(\vec{x}^q)\).
\item Pick sample points around the base point and evaluate \(F\) at those points.
\item Set the new base point \(\vec{x}^{q+1}\) to the point with the lowest value of \(F\). Stop if
  converged or increment \(q\) and go to step~\ref{itm:brute-force-eval}.
\end{enumerate*}

\subsubsection{Global Random Search}

This can be useful to get an understanding of your search space. Otherwise, it's not a good idea in
general.

\begin{enumerate*}
\item \label{itm:random-pick} Pick a random \(\vec{x}^q\) and evaluate \(F(\vec{x}^q)\).
\item If converged (\(n\) or \(F\) cutoff), stop. Otherwise, go to step~\ref{itm:random-pick}.
\end{enumerate*}

\subsubsection{Powell's Method of Conjugate Directions}

\paragraph{Advantages}
\begin{itemize}
\item Faster than a series of univariate moves because it doesn't slow down as much near the
  optimum. (The conjugate search direction generally points toward the optimum.)
\item Minimizes a quadratic function in a finite number of steps.
\end{itemize}

\paragraph{Disadvantages}
\begin{itemize}
\item If a search gains no improvement, conjugacy is lost and the method breaks down.
\item The method slows down near the optimum. If this becomes too extreme, it may be helpful to
  restart the algorithm from the latest point.
\end{itemize}

\paragraph{Methods}
\begin{enumerate*}
\item Set the initial search directions to the identity matrix
  \(S^0
  = \begin{bmatrix}\vec{S}^0_1 & \vec{S}^0_2 & \cdots & \vec{S}^0_n\end{bmatrix}
  = I_n\) and set \(q = 0\).
\item \label{itm:powell-n-searches} For each \(\vec{S}^q_i\) in \(S^q\):
  \begin{enumerate*}
  \item Find the corresponding \(\alpha^*\) to minimize \(F(\vec{x}^q_i + \alpha^* \vec{S}^q_i)\).
  \item Set \(\vec{x}^q_{i+1} = \vec{x}^q_i + \alpha^* \vec{S}^q_i\).
  \end{enumerate*}
\item Calculate the conjugate direction \(\vec{S}^q_\mathrm{c} = \vec{x}^q_n - \vec{x}^q_0\).
\item Find the corresponding \(\alpha^*\) to minimize \(F(\vec{x}^q_n + \alpha^*
  \vec{S}^q_\mathrm{c})\).
\item Set \(\vec{x}^{q+1}_0 = \vec{x}^q_n + \alpha^* \vec{S}^q_\mathrm{c}\).
\item Set the new search matrix \(S^{q+1} =
\begin{bmatrix}
\vec{S}^q_2 & \vec{S}^q_3 & \cdots & \vec{S}^q_n & \vec{S}^q_\mathrm{c}
\end{bmatrix}\).
\item If not converged, increment \(q\) and go to step~\ref{itm:powell-n-searches}.
\end{enumerate*}

\subsection{First Order Methods}

First order methods are usually more efficient than zero order methods, but they need gradient
information and perform poorly when the gradient is not continuous.

\subsubsection{Steepest Descent (Cauchy's Method)}

This method is simple to implement but slows down near the optimum. The method is to always set the
search direction to \(\vec{S}^q = -\del F(\vec{x}^q)\).

\subsubsection{Fletcher--Reeves Conjugate Direction Method}

This method uses steepest descent as a first move, and then uses a conjugate version of successive
gradients for subsequent moves.

\begin{enumerate*}
\item Pick a starting point \(\vec{x}^0\) and set \(q = 0\).
\item Set \(\vec{S}^0 = -\del F(\vec{x}^0)\).
\item Find \(\alpha^*\) and set \(\vec{x}^1 = \vec{x}^0 + \alpha^* \vec{S}^0\). Increment \(q\).
\item \label{itm:fletcher-beta} Set \(\displaystyle \beta^{q-1} = \frac{{\abs{\del F(\vec{x}^q)}}^2}{{\abs{\del F(\vec{x}^{q-1})}}^2}\).
\item Set \(\vec{S}^q = -\del F(\vec{x}^q) + \beta^{q-1} \vec{S}^{q-1}\).
\item Find \(\alpha^*\) and set \(\vec{x}^{q+1} = \vec{x}^q + \alpha^* \vec{S}^q\).
\item Check convergence. If not converged, go to step~\ref{itm:fletcher-beta}.
\end{enumerate*}

\subsection{Newton's Method}

The search direction for Newton's Method is:
\[\vec{S}^q = -{\left[H(\vec{x}^q)\right]}^{-1} \del F(\vec{x}^q)\]
The basic method is to set \(\alpha^* = 1\). As long as \(H(\vec{x}^q)\) is positive definite, this
is a good move. The method can diverge or overshoot the minimum. To reduce overshoots, a modified
update relation is to solve for the \(\alpha^*\) that gives the 1-D minimum for \(F(\vec{x}^q +
\alpha^* \vec{S}^q)\) instead of just setting it to \(1\).

\subsection{Variable Metric Methods (Quasi-Newton)}

These methods iteratively improve their approximation of the Hessian or its inverse. This allows
them to work similarly to Newton's method, but with only first order information.

\subsubsection{Davidon--Fletcher--Powell (DFP) Method}

This method approximates the inverse of the Hessian.

\begin{enumerate*}
\item Set
  \begin{itemize*}
  \item the initial point \(\vec{x}^0\)
  \item the inverse Hessian approximation \(H^0 = I_n\)
  \item the vector \(\vec{c}^0 = \del F(\vec{x}^0)\)
  \item the convergence criterion \(\varepsilon\)
  \item the iteration number \(q = 0\).
  \end{itemize*}
\item \label{itm:dfp-converge} If \(\norm{\vec{c}^q} < \varepsilon\), stop because the method has converged. Otherwise,
  continue to the next step.
\item Set \(\vec{S}^q = -H^q \vec{c}^q\).
\item Find \(\alpha^*\) to minimize \(F(\vec{x}^q + \alpha^* \vec{S}^q)\).
\item Set \(\vec{x}^{q+1} = \vec{x}^q + \alpha^* \vec{S}^q\).
\item Set \(H^{q+1} = H^q + B^q + C^q\) where:
  \begin{align*}
    B^q &= \frac{\vec{p}\transpose{\vec{p}}}{\transpose{\vec{p}}\vec{y}} \\
    C^q &= \frac{-\vec{z}\transpose{\vec{z}}}{\transpose{\vec{y}}\vec{z}} \\
    \vec{p} &= \vec{x}^{q+1} - \vec{x}^q \\
    \vec{y} &= \del F(\vec{x}^{q+1}) - \del F(\vec{x}^q) \\
    \vec{z} &= H^q \vec{y}
  \end{align*}
\item Increment \(q\) and go to step~\ref{itm:dfp-converge}.
\end{enumerate*}

\subsubsection{Broyden--Fletcher--Goldfarb--Shanno (BFGS) Method}

This method approximates the Hessian directly.

\begin{enumerate*}
\item Set
  \begin{itemize*}
  \item the initial point \(\vec{x}^0\)
  \item the Hessian approximation \(H^0 = I_n\)
  \item the vector \(\vec{c}^0 = \del F(\vec{x}^0)\)
  \item the convergence criterion \(\varepsilon\)
  \item the iteration number \(q = 0\).
  \end{itemize*}
\item \label{itm:bfgs-converge} If \(\norm{\vec{c}^q} < \varepsilon\), stop because the method has converged. Otherwise,
  continue to the next step.
\item Solve \(H^q\vec{S}^q = -\vec{c}^q\) for \(\vec{S}^q\).
\item Find \(\alpha^*\) to minimize \(F(\vec{x}^q + \alpha^* \vec{S}^q)\).
\item Set \(\vec{x}^{q+1} = \vec{x}^q + \alpha^* \vec{S}^q\).
\item Set \(H^{q+1} = H^q + D^q + E^q\) where:
  \begin{align*}
    D^q &= \frac{\vec{y}\transpose{\vec{y}}}{\transpose{\vec{y}}\vec{p}} \\
    E^q &= \frac{\vec{c}\transpose{\vec{c}}}{\transpose{\vec{c}}\vec{S}} \\
    \vec{p} &= \vec{x}^{q+1} - \vec{x}^q \\
    \vec{y} &= \del F(\vec{x}^{q+1}) - \del F(\vec{x}^q) \\
  \end{align*}
\item Increment \(q\) and go to step~\ref{itm:bfgs-converge}.
\end{enumerate*}

\begin{thebibliography}{99}
\bibitem{flec} Ferguson, S. M., 2014, \emph{Engineering Design Optimization}, MAE 531 course
  lectures, North Carolina State University, Raleigh, NC.
\bibitem{eotap} Rao, S. S., 2009, \emph{Engineering Optimization: Theory and Practice}, 4th ed.,
  John Wiley \& Sons, Inc., Hoboken, NJ.
\end{thebibliography}

\end{document}
